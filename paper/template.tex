%% Copernicus Publications Manuscript Preparation Template for LaTeX Submissions
%% ---------------------------------
%% This template should be used for copernicus.cls
%% The class file and some style files are bundled in the Copernicus Latex Package, which can be downloaded from the different journal webpages.
%% For further assistance please contact Copernicus Publications at: production@copernicus.org
%% https://publications.copernicus.org/for_authors/manuscript_preparation.html


%% Please use the following documentclass and journal abbreviations for discussion papers and final revised papers.

%% 2-column papers and discussion papers
\documentclass[tc, manuscript]{copernicus}



%% Journal abbreviations (please use the same for discussion papers and final revised papers)


% Advances in Geosciences (adgeo)
% Advances in Radio Science (ars)
% Advances in Science and Research (asr)
% Advances in Statistical Climatology, Meteorology and Oceanography (ascmo)
% Annales Geophysicae (angeo)
% Archives Animal Breeding (aab)
% ASTRA Proceedings (ap)
% Atmospheric Chemistry and Physics (acp)
% Atmospheric Measurement Techniques (amt)
% Biogeosciences (bg)
% Climate of the Past (cp)
% DEUQUA Special Publications (deuquasp)
% Drinking Water Engineering and Science (dwes)
% Earth Surface Dynamics (esurf)
% Earth System Dynamics (esd)
% Earth System Science Data (essd)
% E&G Quaternary Science Journal (egqsj)
% European Journal of Mineralogy (ejm)
% Fossil Record (fr)
% Geochronology (gchron)
% Geographica Helvetica (gh)
% Geoscience Communication (gc)
% Geoscientific Instrumentation, Methods and Data Systems (gi)
% Geoscientific Model Development (gmd)
% History of Geo- and Space Sciences (hgss)
% Hydrology and Earth System Sciences (hess)
% Journal of Micropalaeontology (jm)
% Journal of Sensors and Sensor Systems (jsss)
% Magnetic Resonance (mr)
% Mechanical Sciences (ms)
% Natural Hazards and Earth System Sciences (nhess)
% Nonlinear Processes in Geophysics (npg)
% Ocean Science (os)
% Primate Biology (pb)
% Proceedings of the International Association of Hydrological Sciences (piahs)
% Scientific Drilling (sd)
% SOIL (soil)
% Solid Earth (se)
% The Cryosphere (tc)
% Weather and Climate Dynamics (wcd)
% Web Ecology (we)
% Wind Energy Science (wes)


%% \usepackage commands included in the copernicus.cls:
%\usepackage[german, english]{babel}
%\usepackage{tabularx}
%\usepackage{cancel}
%\usepackage{multirow}
%\usepackage{supertabular}
%\usepackage{algorithmic}
%\usepackage{algorithm}
%\usepackage{amsthm}
%\usepackage{float}
\usepackage{subfig}
%\usepackage{rotating}


\begin{document}

\title{DeepBedMap: Using a deep neural network to better resolve the bed topography of Antarctica}


% \Author[affil]{given_name}{surname}

\Author[1]{Wei Ji}{Leong}
\Author[1]{Huw J.}{Horgan}

\affil[1]{Antarctic Research Centre, Victoria University of Wellington, Wellington, New Zealand}

%% The [] brackets identify the author with the corresponding affiliation. 1, 2, 3, etc. should be inserted.

%% If an author is deceased, please add a further affiliation and mark the respective author name(s) with a dagger, e.g. "\Author[2,$\dag$]{Anton}{Aman}" with the affiliations "\affil[2]{University of ...}" and "\affil[$\dag$]{deceased, 1 July 2019}"

\correspondence{W. J. Leong (weiji.leong@vuw.ac.nz)}

\runningtitle{DeepBedMap Antarctica}

\runningauthor{W. J. Leong and H. J. Horgan}






\received{}
\pubdiscuss{} %% only important for two-stage journals
\revised{}
\accepted{}
\published{}

%% These dates will be inserted by Copernicus Publications during the typesetting process.


\firstpage{1}

\maketitle



\begin{abstract}
To better resolve the bed elevation of Antarctica, we present DeepBedMap - a novel machine learning method that produces realistic Antarctic bed topography from multiple remote sensing data inputs.
Our super-resolution deep convolutional neural network model is trained on scattered regions in Antarctica where high resolution (250 m) groundtruth bed elevation grids are available.
The model is then used to generate high resolution bed topography in less well surveyed areas.
DeepBedMap improves on previous interpolation methods by not restricting itself to a low spatial resolution (1000 m) BEDMAP2 raster image as its prior.
It takes in additional high spatial resolution datasets, such as ice surface elevation, velocity and snow accumulation to better inform the bed topography even in the absence of ice-thickness data from direct ice-penetrating radar surveys.
Our DeepBedMap model is based on an adapted Enhanced Super Resolution Generative Adversarial Network architecture, chosen to minimize per-pixel elevation errors while producing realistic topography.
The final product is a four times upsampled (250 m) bed elevation model of Antarctica that can be used by glaciologists interested in the subglacial terrain, and by ice sheet modellers wanting to run catchment or continent-scale ice sheet model simulations.
We show that DeepBedMap offers a more realistic topographic roughness profile compared to a standard bicubic interpolated BEDMAP2, and envision it to be used alongside other inverse methods to generate the next generation of Antarctic bed elevation models.
\end{abstract}


\copyrightstatement{This work is distributed under the Creative Commons Attribution 4.0 License}


\introduction  %% \introduction[modified heading if necessary]

To create a more detailed map of Antarctica's bed, we present a novel deep convolutional neural network that produces realistic terrain given multiple remote sensing data inputs.
A higher resolution digital bed elevation model of Antarctica enables us to better estimate the volume of ice in Antarctica, and to quantify glacier ice flow dynamics more accurately, particularly along fast flowing ice streams near the grounding line.
This in turn allows for the development of more accurate ice sheet models estimating past and future sea level changes.

There are two techniques for mapping the subglacial terrain of Antarctica.
The main method is via the use of ice-penetrating radar - a method that images the ice-bed interface along survey lines.
This method does come with uncertainties due to different interpretations of radargrams, such as with variations in firn density across sites, and potential errors in the semi-automatic bed picks.
However, this is still the most direct method, though it is geographically limited and involves manual, time consuming work.

A method to overcome the geographical limitations is via an indirect inversion method - using surface data observations to determine bed characteristics.
A complex non-linear relationship exists between the surface elevation and bed elevation of glaciers, ice streams and ice sheets \citep{Raymondrelationshipsurfacebasal2005}, meaning one can theoretically use a well resolved surface to infer bed properties \citep[e.g.][]{Farinottimethodestimateice2009}.
Using surface observation inputs, such as the glacier outline, surface digital elevation models, surface mass balance, surface rate of elevation change, and surface ice flow velocity, various models have been tested in the Ice Thickness Models Intercomparison eXperiment \citep[ITMIX,][]{FarinottiHowaccurateare2017} to determine ice thickness (surface elevation minus bed elevation).
While significant inter-model uncertainties do exist, they can be mitigated by combining several models in an ensemble to provide a better consensus estimate \citep{Farinotticonsensusestimateice2019}.
On a larger scale, the inverse technique has also been applied to the Greenland \citep{MorlighemBedMachinev3Complete2017} and Antarctic \citep{MorlighemDeepglacialtroughs2019} ice sheets, specifically using the mass conservation approach \citep{Morlighemmassconservationapproach2011}.

Here, we present a deep learning method that belongs to the inverse modelling category, but one that is trained on direct ice-penetrating radar observations over Antarctica.
Similar work has been done before using Artificial Feedforward Neural Networks for estimating bed topography \citep[e.g.][]{ClarkeNeuralNetworksApplied2009,MonnierInferencebedtopography2018}, but to our knowledge, none so far in the glaciological community have attempted to use Convolutional Neural Networks that works in a more spatially-aware, 2-dimensional setting.
Convolutional Neural Networks are prevalent in the computer vision community \citep[see][for a review]{LeCunDeeplearning2015}, having existed since the 1980s \citep{FukushimaNeocognitronnewalgorithm1982,LeCunBackpropagationAppliedHandwritten1989} and are commonly used in visual pattern recognition tasks \citep[e.g.][]{LecunGradientbasedlearningapplied1998,KrizhevskyImageNetClassificationDeep2012}.
Our main contributions are twofold:
1) Present a high resolution (250 m) bed elevation map of Antarctica that goes beyond the 1km resolution of BEDMAP2.
2) Design a deep convolutional neural network to integrate as many remote sensing datasets as possible which are relevant for estimating Antarctica's bed topography;
 \citep{FretwellBedmap2improvedice2013}.
We name our neural network "DeepBedMap", and the resulting digital elevation model (DEM) product as "DeepBedMap\_DEM".


\section{Related Work}

\subsection{Super-Resolution}

Super-Resolution involves the processing of a low resolution raster image into a higher resolution one \citep{TsaiMultiframeimagerestoration1984}.
The problem is especially ill-posed because a specific low resolution input can correspond to many possible high resolution outputs, resulting in the development of several different algorithms aimed at solving this challenge \citep[see][for a review]{NasrollahiSuperresolutioncomprehensivesurvey2014}.
One of the most promising approaches is using deep learning \citep{LeCunDeeplearning2015} to learn an end-to-end mapping between the low and high resolution images, a method developed originally in the Super-Resolution Convolutional Neural Network paper \citep[SRCNN,][]{DongImageSuperResolutionUsing2014}.

More efficient neural network architectures and effective optimization objectives have been developed since SRCNN to further improve the quality of results \cite[see][for a review]{YangDeepLearningSingle2018}.
Some key improvements include adding an adversarial loss \citep{GoodfellowGenerativeAdversarialNetworks2014} to produce finer perceptual details as in the Super-Resolution Generative Adversarial Network \citep[SRGAN,][]{LedigPhotoRealisticSingleImage2016}, or by adding residual connections while removing unnecessary model components to enable more efficient training of deeper models as in the Enhanced Deep Super-Resolution network \citep[EDSR,][]{LimEnhancedDeepResidual2017}.
For our DeepBedMap model in this paper, we choose to adapt the Enhanced Super-Resolution Generative Adversarial Network \citep[ESRGAN,][]{WangESRGANEnhancedSuperResolution2018} that brings together the ideas mentioned above to produce state of the art perceptual quality, enough to win the 2018 Perceptual Image Restoration and Manipulation (PIRM) Challenge on Super-Resolution (Third Region) \citep{Blau2018PIRMChallenge2018}.

\subsection{Network Conditioning}

Network conditioning means having a neural network process one source of information in the context of other sources \citep{DumoulinFeaturewisetransformations2018}.
In a geographical context, conditioning is akin to using not just one layer, but several overlapping and semantically meaningful layers so as to better solve the task at hand.
It should be noted that many ways exist to insert extra conditional information into a neural network, such as concatenation-based conditioning, conditional biasing, conditional scaling, and conditional affine transformations \citep{DumoulinFeaturewisetransformations2018}.
We choose to use the concatenation-based conditioning approach, whereby all of the individual raster images are concatenated together channel-wise, much like the individual bands of a multispectral satellite image.
This seemed to be the most appropriate conditioning method as all our contextual remote sensing datasets are raster grid images, and also because it aligns with related work in the remote sensing field.

A nice example similar to our DEM super-resolution problem is the classic problem of pan-sharpening, where a blurry low resolution multispectral image conditioned with a high resolution panchromatic image can be turned into a high resolution multispectral image.
There is ongoing research into the use of deep convolutional neural networks for pan-sharpening \citep{MasiPansharpeningConvolutionalNeural2016,ScarpaTargetAdaptiveCNNBasedPansharpening2018}, sometimes with the incorporation of specific domain-knowledge \citep{YangPanNetDeepNetwork2017}, all of which show promising improvements over classical image processing methods.
More recently, generative adversarial networks \citep{GoodfellowGenerativeAdversarialNetworks2014} have been used in the conditional sense for general image-to-image translation tasks \citep[e.g.][]{IsolaImagetoImageTranslationConditional2016,ParkSemanticImageSynthesis2019}, and also for producing more realistic pan-sharpened satellite images \citep{LiuPSGANGenerativeAdversarial2018}.
Our DeepBedMap model thus builds upon these ideas and other related DEM super-resolution work \citep{XuNonlocalsimilaritybased2015,ChenConvolutionalNeuralNetwork2016}, while incorporating extra conditional information specific to the cryospheric domain for resolving the bed elevation of Antarctica.


\section{Data and Methods}

\subsection{Data Preparation}

Our Convolutional neural network model works on 2D images, so we have to ensure all our datasets are in a suitable raster grid format.
Groundtruth bed elevation points picked from radar surveys (see Table \ref{table:groundtruthdata}) are first compiled together onto a common Antarctic Stereographic Projection (EPSG:3031) using the WGS84 datum, reprojecting if necessary.

These points are then gridded onto a 250 m spatial resolution (pixel-node registered) grid.
We do so by preprocessing the points first, computing the median elevation for each pixel block in a regular grid.
The preprocessed points are then run through an adjustable tension continuous curvature spline function with a tension factor set to 0.35 to produce a digital elevation model grid.
This grid is further post-processed to mask out pixels that are more than 3 pixels (750 m) from the nearest groundtruth point.
These operations are carried out using Generic Mapping Tools v6.0 \citep[GMT6,][]{WesselGenericMappingTools2019}.

\begin{table}[htbp]
  \caption{
    High Resolution groundtruth datasets from ice-penetrating radar surveys (collectively labelled as $y$) used to train our DeepBedMap model.
    Actual training sites can be seen in Figure \ref{fig:2}.
  }
  \label{table:groundtruthdata}
  \begin{tabular}{lcr}
  \tophline
  Location & Citation \\
  \middlehline
  Pine Island Glacier & \cite{BinghamDiverselandscapesPine2017} \\
  Wilkes Subglacial Basin & \cite{JordanHypothesismegaoutburstflooding2010} \\
  Carlson Inlet & \cite{KingIcestreamnot2011} \\
  Rutford Ice Stream & \cite{KingSubglaciallandformsRutford2016} \\
  Various locations in Antarctica & \cite{ShiMultichannelCoherentRadar2010} \\
  \bottomhline
  \end{tabular}
  \belowtable{} % Table Footnotes
\end{table}

\begin{table*}[htbp]
  \caption{Remote Sensing dataset inputs into the DeepBedMap neural network model.}
  \label{table:datainputs}
  \begin{tabular}{cccc}
  \tophline
  Symbol & Name & Spatial Resolution & Citation \\
  \middlehline
  $x$ & BEDMAP2 & 1000 m & \cite{FretwellBedmap2improvedice2013} \\
  $w^1$ & REMA & 100 m* & \cite{HowatReferenceElevationModel2019} \\
  $w^2$ & MEaSUREs Ice Velocity & 500 m** & \cite{MouginotContinentwideinterferometric2019} \\
  $w^3$ & Antarctic Snow Accumulation & 1000 m & \cite{ArthernAntarcticsnowaccumulation2006} \\
  \bottomhline
  \end{tabular}
  \belowtable{
    * gaps in 100 m mosaic filled in with bilinear resampled 200 m resolution REMA image \\
    ** originally 450 m, bilinear resampled to 500 m.
  } % Table Footnotes
\end{table*}

To create our training dataset, we use a sliding window to obtain smaller square tiles from the high resolution (250 m) groundtruth bed elevation grids, with each tile required to be completely filled with data (i.e. no NaN values).
The general location of these training tiles can be seen in Figure \ref{fig:2}.
Besides these groundtruth bed elevation tiles, we also obtain other tiled inputs (see Table \ref{table:datainputs}) corresponding to the same spatial bounding box area.
To reduce border edge effects, note that we have used no padding (also known as 'valid' padding) in our neural network model's input convolutional layers (see Figure \ref{fig:1}) which means our model input grids ($x$, $w^1$, $w^2$, $w^3$) will cover a larger spatial area than the groundtruth grids ($y$).
More specifically, the model inputs cover an area of 11x11 km (e.g. 11x11 pixels for BEDMAP2) while the groundtruth grids cover an area of 9x9 km (36x36 pixels).
As the pixels of our groundtruth grids may not align perfectly with that of our model input grids, we use bilinear interpolation to ensure that all our input grids cover the same spatial bounds as that of our reference groundtruth tiles.

\subsection{Model Design}

Our DeepBedMap model is a Generative Adversarial Network \citep{GoodfellowGenerativeAdversarialNetworks2014} composed of two neural network models, a Generator $G_\theta$ that produces the bed elevation prediction, and a Discriminator $D_\eta$ critic that will judge the quality of this output.
The two models are trained to compete against each other, with the Generator producing ever more accurate images to fool the Discriminator, and the Discriminator learning to spot problems with the Generator's prediction in relation to the groundtruth.
Following this is a mathematical definition of the neural network models and their architecture.

The objective of our 4 times upsampling super-resolution model is to produce a high resolution (250 m) grid of Antarctica's bed elevation $\hat{y}$ given a low resolution (1000 m) BEDMAP2 \citep{FretwellBedmap2improvedice2013} image $x$.
However, the information contained in BEDMAP2 is insufficient for this regular super-resolution task, so we provide the neural network with more context through network conditioning.
Specifically, the model is conditioned at the input block stage with three raster grids (see Table \ref{table:datainputs}): 1) ice surface elevation $w^1$, 2) ice surface velocity $w^2$, and 3) snow accumulation $w^3$.
This can be formulated as follows:

\begin{equation}\label{eq:1}
  \hat{y} = G_\theta(x, w^1, w^2, w^3)
\end{equation}

where $G_\theta$ is the Generator (see Figure \ref{fig:1}) that produces high resolution image candidates $\hat{y}$.
For brevity in the following equations, we simplify Equation \eqref{eq:1} to hide conditional inputs $w^1, w^2, w^3$, i.e. represent all input images using $x$.
To train our Generative Adversarial Network, we update the parameters of our Generator $\theta$ and Discriminator $\eta$ as follows:

\begin{align}
  & \hat{\theta} = \arg\min_{\theta} \frac{1}{N}\sum_{n=1}^{N}L_G(\hat{y}_n, y_n) \label{eq:2}\\
  & \hat{\eta} = \arg\min_{\eta} \frac{1}{N}\sum_{n=1}^{N}L_D(\hat{y}_n, y_n) \label{eq:3}
\end{align}

where $L_G$ and $L_D$ are the total loss functions of the Generator $G$ and Discriminator $D$ that we want to minimize, and $\hat{y}_n$, $y_n$ are the set of predicted and groundtruth high resolution images over $N$ training samples.
The generator network's loss $L_G$ is a custom perceptual loss function with four weighted components - content, adversarial, topographic and structural loss.
The discriminator network's loss $L_D$ is designed to maximize the likelihood that predicted images are classified as fake (0) and groundtruth images are classified as real (1).
The details of these loss functions are described more fully in Appendix \ref{appendix:A}.

Noting that the objective of the Generator $G$ is opposite to that of the Discriminator $D$, we can thus formulate our adversarial min-max problem as follows:

\begin{equation}\label{eq:4}
  \min_{G} \max_{D} V(G,D) = \mathbb{E}_{y \sim P_{\text{data}}(y)}[\ln D(y)] + \mathbb{E}_{x \sim P_{G(x)}}[\ln(1-D(G(x)))]
\end{equation}

where $P_{\text{data}}(y)$ and $P_{G(x)}$ are the empirical distributions over the high resolution groundtruth $y$ and low resolution input images $x$ respectively.
The goal therefore, is to make these distributions as similar as possible through optimizing the value function $V$.

\begin{figure*}[htbp]
  \includegraphics[width=\textwidth]{figures/fig1_deepbedmap_architecture.pdf}
  \caption{
    DeepBedMap Generator model architecture.
    The input module on the left is made up of 4 sub-networks, each one composed of a convolutional neural network that processes the input image into a consistent 9x9 shaped tensor.
    From top to bottom, we have our prior low resolution input BEDMAP2 \citep{FretwellBedmap2improvedice2013}, and conditional inputs REMA \citep{HowatReferenceElevationModel2019}, MEaSUREs Ice Veloctiy \citep{MouginotContinentwideinterferometric2019}, and Snow Accumulation \citep{ArthernAntarcticsnowaccumulation2006}, more details of which can be found in Table \ref{table:datainputs}.
    Note that the MEaSUREs Ice Velocity input has two channels, one each for the x and y velocity components.
    All the processed inputs are then concatenated together channel-wise before being fed into the core module.
    The core module in the middle is based on the ESRGAN architecture with 12 Residual-in-Residual Dense Blocks \citep[see][for details]{WangESRGANEnhancedSuperResolution2018}, saddled in between a pre-residual and post-residual convolutional layer.
    A skip connection runs from after the pre-residual layer, to after the post-residual convolution layer.
    The upsampling module on the right is composed of two upsampling blocks (nearest neighbour upsampling followed by a convolutional layer and LeakyReLU activation) that progressively scales our tensors by 2x each time.
    Following this are two deformable convolutional layers \citep{DaiDeformableConvolutionalNetworks2017} which produces our final output super resolution DeepBedMap\_DEM.
  }
  \label{fig:1}
\end{figure*}

DeepBedMap's model architecture is adapted from the Enhanced Super Resolution Generative Adversarial Network \citep[ESRGAN,][]{WangESRGANEnhancedSuperResolution2018}, with a few notable differences.
Specifically, the Generator model $G$ has been modified to:
1) Use a custom conditional input block (see Figure \ref{fig:1}, focusing on input module) consisting of four sub-networks, each one having a different convolution size and stride so as to create identical sized outputs, all of which are then concatenated together channel-wise before being fed into the main body of the ESRGAN-based convolutional neural network (see Figure \ref{fig:1}, focusing on core module).
2) Use Deformable Convolution \citep{DaiDeformableConvolutionalNetworks2017} instead of standard Convolution in the final two layers (see Figure \ref{fig:1}, focusing on upsampling module) to enhance our model's predictive capability by having it learn dense spatial transformations.
This Generator model is trained to gradually improve its prediction by comparing the predicted output with groundtruth images in our training regions (see Figure \ref{fig:2}), using the total loss function defined in Equation \eqref{eq:A9}.

Besides the Generator model, there is a separate adversarial Discriminator model $D$ (not shown).
Again, we follow ESRGAN's \citep{WangESRGANEnhancedSuperResolution2018} lead by implementating our adversarial Discriminator network in the style of the Visual Geometry Group (VGG) convolutional neural network model \citep{SimonyanVeryDeepConvolutional2014}.
The Discriminator model consists of 10 Convolutional-BatchNormalization-LeakyReLU blocks, followed by two fully-connected layers of 100 and 1 neurons respectively.
For numerical stability, we omit the final fully-connected layer's sigmoid activation function from the Discriminator model's construction, integrating it instead into the binary cross entropy loss functions at Equation \eqref{eq:A2} and Equation \eqref{eq:A3} using the log-sum-exp trick.
The output of this Discriminator model is a floating point number ranging from 0 (fake) to 1 (real) that scores the Generator model's output image.
This score is used by both the Discriminator and Generator in the training process, and helps to push our predictions towards more realistic bed elevation surface textures.


\section{Results}

\subsection{DeepBedMap\_DEM Topography}

\begin{figure*}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{figures/fig2_deepbedmap_dem.png}
    \caption{
      DeepBedMap\_DEM over the entire Antarctic continent.
      Plotted on an Antarctic Stereographic Projection (EPSG:3031) with elevation referenced to the WGS84 datum.
      Grounding line is plotted as thin black line.
      Purple box shows Pine Island Glacier extent used in Figure \ref{fig:3}.
      Yellow box shows Thwaites Glacier extent used in Figure \ref{fig:4}
      Orange areas show locations of training tiles (see Table \ref{table:groundtruthdata}).
    }
    \label{fig:2}
\end{figure*}

Here we present the output Digital Elevation Model (DEM) of our super-resolution DeepBedMap neural network model, and compare it with bed topography produced by other methods.
The resulting DEM has a 250 m spatial resolution, and is essentially a four-times upsampled bed elevation grid product of BEDMAP2 \citep{FretwellBedmap2improvedice2013}.
In Figure \ref{fig:2}, we show that the full Antarctic-wide DeepBedMap\_DEM manages to capture general topographical features across the whole continent.
The model is only valid for grounded ice regions, but we have produced predictions extending outside of the grounding zone area (including ice shelf cavities) for visual purposes as well as to make it easier to integrate with other grid products.

\begin{figure}[htbp]
  \includegraphics[width=0.75\columnwidth]{figures/fig3_qualitative_bed_comparison.png}
  \caption{
    Comparison of interpolated bed elevation grid products over Pine Island Glacier (see extent in Figure \ref{fig:2}).
    \textbf{a} DeepBedMap (ours) at 250 m resolution.
    \textbf{b} BEDMAP2 \citep{FretwellBedmap2improvedice2013}, originally 1000 m, bicubic interpolated to 250 m.
    \textbf{c} Elevation Difference between DeepBedMap and BEDMAP2.
    \textbf{d} Bedmachine Antarctica \citep{MorlighemMEaSUREsBedMachineAntarctica2019}, originally 500 m, bicubic interpolated to 250 m.
  }
  \label{fig:3}
\end{figure}

We highlight a few qualitative observations on closer visual inspection of a zoomed-in DeepBedMap\_DEM.
Over relatively flat regions in both West and East Antarctica, there are megadune-like structures \citep{ScambosSnowMegadune2014} in our DeepBedMap\_DEM that our DeepBedMap model has likely picked up from using the high resolution ice surface elevation model.
Over steep topographical areas such as the Transantarctic Mountains and Gamburtsev Subglacial Mountains, DeepBedMap produced wisp-like texture patterns (see Appendix \ref{appendix:C}).
Still, our DeepBedMap\_DEM shows a terrain with strikingly realistic topographical features (see Figure \ref{fig:3}), being rougher than that of BEDMAP2 \citep{FretwellBedmap2improvedice2013} and Bedmachine Antarctica \citep{MorlighemMEaSUREsBedMachineAntarctica2019}, yet still preserving the general topography of the area.

Following this, we make some quantitative bed roughness comparisons between our DeepBedMap\_DEM, BEDMAP2, and BedMachine Antarctica \citep{MorlighemMEaSUREsBedMachineAntarctica2019}.

\subsection{Surface Roughness}

One of the key challenges when producing a high resolution Digital Elevation Model (DEM) of Antarctica's bed elevation surface is the need to preserve a realistic rough terrain.
Bed roughness, especially at short-wavelengths, exerts a frictional force on ice which has an important influence on ice velocity \citep{BinghamDiverselandscapesPine2017,FalciniQuantifyingbedroughness2018}.

Using standard deviation as a simple measure of roughness \citep{RippinBasalroughnessInstitute2014}, we show a quick intercomparison of the different grids over Thwaites Glacier spatially in 2D (see Figure \ref{fig:4}) as well as over a 1D transect (see Figure \ref{fig:5}).
Specifically, the surface roughness for a single 250 m pixel is calculated from the standard deviation of elevation values over a square 1250 m area (i.e. 5 pixels) surrounding the centre of the pixel.
The comparisons are made between our DeepBedMap\_DEM and BEDMAP2 in relation to groundtruth grids from processed Operation IceBridge data.

\begin{figure*}[htbp]
  \centering
  \subfloat[DeepBedMap Digital Elevation Model (DEM).]{
    \includegraphics[width=0.4\textwidth]{figures/fig4a_elevation_deepbedmap.png}
    \label{fig:4a}
  }
  \subfloat[2D roughness from our DeepBedMap\_DEM grid.]{
    \includegraphics[width=0.4\textwidth]{figures/fig4b_roughness_deepbedmap.png}
    \label{fig:4b}
  }
  \qquad
  \subfloat[2D roughness from interpolated Operation IceBridge grid.]{
    \includegraphics[width=0.4\textwidth]{figures/fig4c_roughness_groundtruth.png}
    \label{fig:4c}
  }
  \subfloat[2D roughness from bicubic interpolated BEDMAP2 grid.]{
    \includegraphics[width=0.4\textwidth]{figures/fig4d_roughness_bedmap2.png}
    \label{fig:4d}
  }
  \caption{
    Spatial 2D view of grids over Thwaites Glacier, West Antarctica.
    Plotted on an Antarctic Stereographic Projection (EPSG:3031) with elevation and standard deviation values in metres referenced to the WGS84 datum.
    Orange groundtruth points in \textbf{a} correspond to transect sampling locations used in Figure \ref{fig:5}.
    Note that color scale of \textbf{b} and \textbf{c} is twice that of \textbf{d}.
  }
  \label{fig:4}
\end{figure*}

\begin{figure}[htbp]
    \includegraphics[width=0.9\columnwidth]{figures/fig5_roughness_transect.png}
    \caption{
      Comparing surface roughness (standard deviation of elevation values) of each interpolated grid product (250 m resolution) over a transect.
      Orange values are from tension spline interpolated Operation IceBridge groundtruth points;
      Purple values are from our super resolution DeepBedMap\_DEM;
      Green values are from bicubic interpolated BEDMAP2.
    }
    \label{fig:5}
\end{figure}


\section{Discussion}

In the results section, we show that our DeepBedMap\_DEM v1 produces a high resolution (250 m) result (see Figure \ref{fig:3}) that can capture a detailed and realistic picture of the underlying bed topography.
We also quantify that a well trained DeepBedMap neural network model can produce lower errors in roughness than existing interpolation and inverse modelling efforts (see Figure \ref{fig:4} and \ref{fig:5}).

Our DeepBedMap model is trained only on a small fraction of the area of Antarctica, simply because the Convolutional Neural Network cannot be trained on sparse survey point measurements.
The topography generated by our model is quite sensitive to the accuracy of its data inputs (see Table \ref{table:groundtruthdata} and \ref{table:datainputs}), and though this is a problem faced by many other inverse methods, neural network models like ours can be particularly biased towards the training dataset.
In particular, our experimental model's topography is likely skewed towards the distribution of our training regions that tend to reside in coastal regions, especially over ice streams in West Antarctica (see Figure \ref{fig:2}).

While care has been taking to source the best possible datasets (see Table \ref{table:groundtruthdata} and \ref{table:datainputs}), we note that there are some limitations and room to improve our DeepBedMap\_DEM results.
Some of the conditional datasets we use such as REMA \citep{HowatReferenceElevationModel2019} and MEaSUREs Ice Velocity \citep{RignotMEaSUREsInSARBasedAntarctica2017} contain data gaps which introduce artifacts in our DeepBedMap\_DEM, and those holes need to be patched up for proper continent-wide prediction.
Another limitation is that the DeepBedMap model currently relies on snapshot data collected over different epochs and has no proper sense of time.
Ice elevation change captured using satellite altimeters (e.g. from ICESat-2 \citep{MarkusIceCloudland2017}) could be added as an additional input to better account for temporal factors.

Furthermore, it is possible to apply our super resolution DeepBedMap technique on bed elevation inputs newer than BEDMAP2 \citep{FretwellBedmap2improvedice2013}, such as the 1000 m resolution DEM over the Weddell Sea \citep{Jeofry1KmBedTopography2017} or the 500 m resolution Bedmachine Antarctica dataset \citep{MorlighemMEaSUREsBedMachineAntarctica2019}.
The DeepBedMap model can be retrained from scratch, retuned, or readapted to handle new datasets that will further improve its predictive performance and produce an even more accurate and higher resolution DeepBedMap\_DEM.


\conclusions  %% \conclusions[modified heading if necessary]

Our DeepBedMap convolutional neural network method presents a purely data-driven approach to resolve the bed topography of Antarctica.
It is an improvement beyond simple interpolation techniques, generating realistic high spatial resolution (250 m) topography that preserves detail in bed roughness and is adaptable for catchment to continent-scale studies on ice sheets.
Unlike other inverse methods that rely on some explicit parameterization of ice-flow physics, our model uses deep learning to find suitable neural network parameters via an iterative error minimization approach.

Moving forward, we have identified a strong need for densely spaced bed elevation datasets sampled over a more diverse portion of Antarctica.
There is a need for denser, high resolution surveys (<250 m flight spacing) that capture a more representative picture of Antarctica's varied bed topography.
Using Graphical Processing Units (GPUs) for training and inference allows our neural network method to scale easily, and the addition of more training datasets will allow it to perform better.

The work here is not meant to discourage the usage of hand designed inverse modelling techniques, but to introduce an independent methodology, with an outlook towards combining the strengths of the two.
Once properly trained, our DeepBedMap model runs quickly and produces realistic rough topography, which when merged with more physically based mass conservation inverse approaches \citep[e.g.][]{MorlighemDeepglacialtroughs2019} will likely mean an even more efficient way of generating accurate bed elevation maps of Antarctica for ice sheet modellers.
One side product resulting from this work is a test-driven development framework that can be used to measure and compare the performance of upcoming bed terrain models.
Indeed, the radioglaciology community has begun to compile a more comprehensive bed elevation/ice thickness dataset for Antarctica, and there has been discussions to combine various terrain interpolation techniques in an ensemble to collaboratively create the new BEDMAP3.




%% The following commands are for the statements about the availability of data sets and/or software code corresponding to the manuscript.
%% It is strongly recommended to make use of these sections in case data sets and/or software code have been part of your research the article is based on.

\codeavailability{
  Python code for data preparation, neural network model training and visualization of model outputs are freely available at https://github.com/weiji14/deepbedmap.
  Experimental model runs are also recorded at https://www.comet.ml/weiji14/deepbedmap.
} %% use this section when having only software code available


\dataavailability{
  Pine Island Glacier dataset \citep{BinghamDiverselandscapesPine2017} available on request from Robert Bingham.
  Carlson Inlet dataset \citep{KingIcestreamnot2011} available on request from Edward King.
  Bed elevation datasets from Wilkes Subglacial Basin \citep{FerraccioliAirborneradarbed2018} and Rutford Ice Stream \citep{KingSubglaciallandformsRutford2016} available from British Antarctic Survey's Polar Data Centre (https://ramadda.data.bas.ac.uk).
  Other Antarctic bed elevation datasets available from Center for Remote Sensing of Ice Sheets (https://data.cresis.ku.edu/data/rds) or from National Snow and Ice Data Center (https://nsidc.org/data/IRMCR2/versions/1).
  BEDMAP2 \citep{FretwellBedmap2improvedice2013} and REMA \citep{HowatIanReferenceElevationModel2018} available from Polar Geospatial Center (http://data.pgc.umn.edu).
  MEaSUREs ice velocity data \citep{MouginotMEaSUREsPhaseMap2019} available from NSIDC (https://nsidc.org/data/nsidc-0754/versions/1).
  Antarctic Snow Accummulation data \citep{ArthernAntarcticsnowaccumulation2006} available from British Antarctic Survey (https://secure.antarctica.ac.uk/data/bedmap2/resources/Arthern\_accumulation).
} %% use this section when having only data sets available


\codedataavailability{TEXT} %% use this section when having data sets and software code available


\sampleavailability{TEXT} %% use this section when having geoscientific samples available


\videosupplement{TEXT} %% use this section when having video supplements available


\appendix

\section{Details of loss function components} \label{appendix:A}

The loss function, or cost function, is a mathematical function that maps a set of input variables to an output loss value.
The loss value can be thought of as a weighted sum of several error metrics between the neural network's prediction and the expected output or groundtruth.
It is this loss value which we want to minimize so as to train our neural network model to perform better, and we do this by iteratively optimizing the parameters in our loss function.
Following this are the details of the various loss functions that make up the total loss function of our DeepBedMap Generative Adversarial Network.

\subsection{Content Loss}

To bring the pixel values of our generated images closer to that of the groundtruth, we first define our Content Loss function $L_1$.
Following ESRGAN \citep{WangESRGANEnhancedSuperResolution2018}, we have:

\begin{equation}\label{eq:A1}
  L_1 = \dfrac{1}{n} \sum\limits_{i=1}^n ||\hat{y}_i - y_i||_{1}
\end{equation}

where we take the mean absolute error between the Generator Network's predicted value $\hat{y}_i$ and the groundtruth value $y_i$, respectively over every pixel $i$.

\subsection{Adversarial Loss}

Next, we define an Adversarial Loss to encourage the production of high resolution images $\hat{y}$ closer to the manifold of natural looking digital elevation model images.
To do so, we introduce the standard discriminator in the form of $D(y) = \sigma(C(y))$ where $\sigma$ is the sigmoid activation function and $C(y)$ is the raw, non-transformed output from a discriminator neural network acting on high resolution image $y$.
The ESRGAN model \citep{WangESRGANEnhancedSuperResolution2018} however, employs an improved Relativistic-average Discriminator \citep{Jolicoeur-Martineaurelativisticdiscriminatorkey2018} denoted by $D_{Ra}$.
It is defined as $D_{Ra}(y,\hat{y}) = \sigma(C(y) - \mathbb{E}_{\hat{y}}[C(\hat{y})])$, where $\mathbb{E}_{\hat{y}}[\cdot]$ is the arithmetic mean operation carried out over every generated image $\hat{y}$ in a mini-batch.
We use a binary cross entropy loss as the discriminator's loss function defined as follows:

\begin{equation}\label{eq:A2}
  L_D^{Ra} = - \mathbb{E}_y[\ln(D(y,\hat{y}))] - \mathbb{E}_{\hat{y}}[\ln(1 - D(\hat{y},y))]
\end{equation}

The generator network's adversarial loss is in a symmetrical form:

\begin{equation}\label{eq:A3}
  L_G^{Ra} = - \mathbb{E}_y[\ln(1 - D(y,\hat{y}))] - \mathbb{E}_{\hat{y}}[\ln(D(\hat{y},y))]
\end{equation}

\subsection{Topographic Loss}

We further define a Topographic Loss so that the elevation values in our super resolved image make topographic sense with respect to the original low resolution image.
Specifically, we want the mean value of each 4x4 grid on the predicted super resolution (DeepBedMap) image to closely match its spatially corresponding 1x1 pixel on the low resolution (BEDMAP2) image.

First, we apply a 4x4 Mean Pooling operation on our Generator Network's predicted super resolution image:

\begin{equation}\label{eq:A4}
 \bar{\hat{y}}_j = \dfrac{1}{n} \sum\limits_{i=1}^n \hat{y}_i
\end{equation}

where $\bar{\hat{y}}_j$ is the mean of all predicted values $\hat{y}_i$ across the 16 super-resolved pixels $i$ within a 4x4 grid corresponding to the spatial location of one low resolution pixel at position $j$.
Following this, we can compute our Topographic Loss as follows:

\begin{equation}\label{eq:A5}
  L_T = \dfrac{1}{m} \sum\limits_{i=1}^m ||\bar{\hat{y}}_j - x_j||_{1}
\end{equation}

where we take the mean absolute error between the mean of the 4x4 super-resolved pixels calculated in Equation \eqref{eq:A4} $\bar{\hat{y}}_j$ and that of the spatially corresponding low resolution pixel $x_j$, respectively over every low resolution pixel $j$.

\subsection{Structural Loss}

Lastly, we define a Structural Loss that takes into account luminance, contrast and structural information between our predicted and groundtruth images.
This is based on the Structural Similarity Index \citep[SSIM,][]{WangImageQualityAssessment2004} and is calculated over a single window patch as so:

\begin{equation}\label{eq:A6}
  SSIM(\hat{y}, y) = \dfrac{(2\mu_{\hat{y}}\mu_y + c_1)(2\sigma_{{\hat{y}}y} + c_2)}{(\mu_{\hat{y}}^2 + \mu_y^2 + c_1)(\sigma_{\hat{y}}^2 + \sigma_y^2 + c_2)}
\end{equation}

where $\mu_{\hat{y}}$ and $\mu_y$ are the arithmetic mean of predicted image ${\hat{y}}$ and groundtruth image $y$ respectively over a single window that we set to 9x9 pixels, $\sigma_{{\hat{y}}y}$ is the covariance of ${\hat{y}}$ and $y$, $\sigma_{\hat{y}}^2$ and $\sigma_y^2$ are the variance of ${\hat{y}}$ and $y$ respectively, and $c_1$ and $c_2$ are two variables set to $0.01^2$ and $0.03^2$ to stabilize division with a weak denominator.
Thus, we can formulate our Structural Loss as follows:

\begin{equation}\label{eq:A7}
  L_S = 1 - \dfrac{1}{p} \sum\limits_{i=1}^p SSIM(\hat{y}, y)_p
\end{equation}

where we do $1$ minus the mean of all structural similarity values $SSIM(\hat{y}, y)$ calculated over every patch $p$ obtained via a sliding window over our predicted image ${\hat{y}}$ and groundtruth image $y$.

\subsection{Total Loss Function}

Finally, we compile the loss functions for our discriminator and generator networks as follows:

\begin{align}
  & L_D = L_D^{Ra} \label{eq:A8}\\
  & L_G = \eta L_1 + \lambda L_G^{Ra} + \theta L_T + \zeta L_S \label{eq:A9}
\end{align}

where $\eta$, $\lambda$, $\theta$, and $\zeta$ are the scaled weights for the content $L_1$, adversarial $L_D$, topographic $L_T$ and structural losses $L_S$ respectively (see Table \ref{table:B1} for values used).
The loss functions $L_D$ and $L_G$ are minimized in an alternate 1:1 manner so as to solve the entire Generative Adversarial Network's objective function defined in Equation \eqref{eq:4}.


\section{Neural Network Training Details} \label{appendix:B}

The neural networks were developed using Chainer v7.0.0b2 \citep{TokuiChainerDeepLearning2019}, and trained using full precision (floating point 32) arithmetic.
Experiments were carried out on 4 Graphical Processing Units (GPUs), specifically 2 Tesla P100 GPUs and 2 Tesla V100 GPUs.
On our Tesla V100 GPU setup, one training run with about 150 epochs takes about 30 minutes.
This is using a batch size of 128 on a total of 3826 training image tiles, with 202 tiles reserved for validation, i.e. a 95/5 training/validation split.
We next describe the method used to evaluate each DeepBedMap candidate model, as well as the high-level way in which we semi-automatically arrived at a good model via semi-automatic hyperparameter tuning.

\begin{table*}[htbp]
  \caption{Optimized Hyperparameter Settings.}
  \label{table:B1}
  \begin{tabular}{lrr}
  \tophline
  Hyperparameter & Setting & Tuning Range \\
  \middlehline
  Learning rate (for both Generator and Discriminator) & 1.7e-4 & 2e-4 to 1e-4 \\
  Number of Residual-in-Residual Blocks & 12 & 8 to 14 \\
  Mini-batch size & 128 & 64 or 128 \\
  Number of epochs & 140 & 90 to 150 \\
  Residual scaling & 0.2 & 0.1 to 0.5 \\
  Content Loss Weighting $\eta$ & 1e-2 & Fixed \\
  Adversarial Loss Weighting $\lambda$ & 2e-2 & Fixed \\
  Topographic Loss Weighting $\theta$ & 2e-3 & Fixed \\
  Structural Loss Weighting $\zeta$ & 5.25 & Fixed \\
  He Normal Initialization Scaling & 0.1 & Fixed \\
  Adam optimizer epsilon & 0.1 & Fixed \\
  Adam optimizer beta1 & 0.9 & Fixed \\
  Adam optimizer beta2 & 0.99 & Fixed \\
  \bottomhline
  \end{tabular}
  \belowtable{} % Table Footnotes
\end{table*}

To check for overfitting, we evaluate the Generative Adversarial Network model on our validation dataset after each epoch using two performance metrics - a peak signal-to-noise ratio (PSNR) metric for the Generator, and an accuracy metric for the Discriminator.
Training stops when these validation performance metrics show little improvement, roughly at 120 epochs.

Next, we conduct a full evaluation on an independent test dataset, comparing the model's predicted grid output against actual groundtruth xyz points.
Using the 'grdtrack' function in Generic Mapping Tools v6.0 \citep{WesselGenericMappingTools2019}, we obtain the grid elevation at each of our groundtruth point and use it to calculate the elevation error on a point-to-point basis.
All of these elevation errors are then used to compute a Root Mean Squared Error (RMSE) statistic over this independent test site.
This RMSE value is used to judge our model's performance in relation to baseline bicubic interpolation, and also the metric minimized by our hyperparameter optimization algorithm which we will describe next.

Neural networks contain a lot of hyperparameter settings that need to be decided upon, and Generative Adversarial Networks are particularly sensitive to different hyperparameter settings.
To stabilize model training and obtain better performance, we tune our hyperparameters (see Table \ref{table:B1}) using a Bayesian approach.
Specifically, we employ the Tree-structured Parzen Estimator \citep{BergstraAlgorithmsHyperparameterOptimization2011} from the Optuna v0.14.0 \citep{AkibaOptunaNextgenerationHyperparameter2019} library with default settings as per the Hyperopt library \citep{BergstraHyperoptPythonlibrary2015}.
Given that we have 4 GPUs, we choose to parallelize the hyperparameter tuning experiments asynchronously between all four devices.
The estimator first conducts 20 random experimental trials to scan the hyperparameter space, gradually narrowing down to a few candidate hyperparameters in subsequent experiments.
We set each GPU to run a target of 30 experimental trials (i.e. a total of 120), though unpromising trials that have exploding/vanishing gradients are pruned prematurely to save on time and computational resources.
The top models from these experiments undergo further visual evaluation, and we continue to conduct further experiments until a suitable candidate model is found.


\section{Closeup images of DeepBedMap\_DEM} \label{appendix:C}

\begin{figure*}[htbp]
  \centering
  \subfloat[Scott Glacier]{
    \includegraphics[width=0.3\textwidth]{figures/figc1a_deepbedmap_closeup.png}
    \label{fig:C1a}
  }
  \subfloat[Whillans Ice Stream]{
    \includegraphics[width=0.3\textwidth]{figures/figc1b_deepbedmap_closeup.png}
    \label{fig:C1b}
  }
  \subfloat[Echelmeyer Ice Stream]{
    \includegraphics[width=0.3\textwidth]{figures/figc1c_deepbedmap_closeup.png}
    \label{fig:C1c}
  }
  \qquad
  \subfloat[Evans Ice Stream]{
    \includegraphics[width=0.3\textwidth]{figures/figc1d_deepbedmap_closeup.png}
    \label{fig:C1d}
  }
  \subfloat[Rutford Ice Stream]{
    \includegraphics[width=0.3\textwidth]{figures/figc1e_deepbedmap_closeup.png}
    \label{fig:C1e}
  }
  \subfloat[Foundation Ice Stream]{
    \includegraphics[width=0.3\textwidth]{figures/figc1f_deepbedmap_closeup.png}
    \label{fig:C1f}
  }
  \qquad
  \subfloat[Totten Glacier]{
    \includegraphics[width=0.3\textwidth]{figures/figc1g_deepbedmap_closeup.png}
    \label{fig:C1g}
  }
  \subfloat[Byrd Glacier]{
    \includegraphics[width=0.3\textwidth]{figures/figc1h_deepbedmap_closeup.png}
    \label{fig:C1h}
  }
  \subfloat[Gamburtsev Subglacial Mountains]{
    \includegraphics[width=0.3\textwidth]{figures/figc1i_deepbedmap_closeup.png}
    \label{fig:C1i}
  }
  \caption{
    Closeup views of DeepBedMap\_DEM around Antarctica.
    Top row shows Ross Sea region locations along the Transantarctic Mountains, Siple Coast and Shirase Coast.
    Middle row shows Weddell Sea region locations.
    Bottom row shows East Antarctica locations.
  }
  \label{fig:C}
\end{figure*}

\noappendix       %% use this to mark the end of the appendix section

%% Regarding figures and tables in appendices, the following two options are possible depending on your general handling of figures and tables in the manuscript environment:

%% Option 1: If you sorted all figures and tables into the sections of the text, please also sort the appendix figures and appendix tables into the respective appendix sections.
%% They will be correctly named automatically.

%% Option 2: If you put all figures after the reference list, please insert appendix tables and figures after the normal tables and figures.
%% To rename them correctly to A1, A2, etc., please add the following commands in front of them:

\appendixfigures  %% needs to be added in front of appendix figures

\appendixtables   %% needs to be added in front of appendix tables

%% Please add \clearpage between each table and/or figure. Further guidelines on figures and tables can be found below.



\authorcontribution{
  W. J. L. - conceptualization, data curation, formal analysis, methodology, software, visualization, writing  original draft, writing  review \& editing.
  H. J. H. - funding acquisition, supervision, writing  review \& editing.
} %% this section is mandatory

\competinginterests{The authors declare that they have no conflict of interest.} %% this section is mandatory even if you declare that no competing interests are present

\disclaimer{TEXT} %% optional section

\begin{acknowledgements}
  We are grateful to Robert Bingham and Edward King for the Pine Island Glacier and Carlson Inlet data, and to all the other researchers in the British Antarctic Survey and Operation IceBridge team for providing free access to the high resolution bed elevation datasets around Antarctica.
  A special thanks to Ruzica Dadic for her help in reviewing draft versions of this paper.
  This research was funded by the Royal Society of New Zealand's Rutherford Discovery Fellowship (Contract: RDFVUW1602), with additional support from the Erasmus+ programme and International Glaciological Society early career travel award for presenting earlier versions of this work at the 2019 EGU General Assembly and IGS Symposium on Five Decades of Radioglaciology.
\end{acknowledgements}




%% REFERENCES

%% The reference list is compiled as follows:

%% \begin{thebibliography}{}

%% \bibitem[AUTHOR(YEAR)]{LABEL1}
%% REFERENCE 1

%% \bibitem[AUTHOR(YEAR)]{LABEL2}
%% REFERENCE 2

%% \end{thebibliography}

%% Since the Copernicus LaTeX package includes the BibTeX style file copernicus.bst,
%% authors experienced with BibTeX only have to include the following two lines:
%%
\bibliographystyle{copernicus}
\bibliography{example.bib}
%%
%% URLs and DOIs can be entered in your BibTeX file as:
%%
%% URL = {http://www.xyz.org/~jones/idx_g.htm}
%% DOI = {10.5194/xyz}


%% LITERATURE CITATIONS
%%
%% command                        & example result
%% \citet{jones90}|               & Jones et al. (1990)
%% \citep{jones90}|               & (Jones et al., 1990)
%% \citep{jones90,jones93}|       & (Jones et al., 1990, 1993)
%% \citep[p.~32]{jones90}|        & (Jones et al., 1990, p.~32)
%% \citep[e.g.,][]{jones90}|      & (e.g., Jones et al., 1990)
%% \citep[e.g.,][p.~32]{jones90}| & (e.g., Jones et al., 1990, p.~32)
%% \citeauthor{jones90}|          & Jones et al.
%% \citeyear{jones90}|            & 1990



%% FIGURES

%% When figures and tables are placed at the end of the MS (article in one-column style), please add \clearpage
%% between bibliography and first table and/or figure as well as between each table and/or figure.


%% ONE-COLUMN FIGURES

%%f
%\begin{figure}[t]
%\includegraphics[width=8.3cm]{FILE NAME}
%\caption{TEXT}
%\end{figure}
%
%%% TWO-COLUMN FIGURES
%
%%f
%\begin{figure*}[t]
%\includegraphics[width=12cm]{FILE NAME}
%\caption{TEXT}
%\end{figure*}
%
%
%%% TABLES
%%%
%%% The different columns must be seperated with a & command and should
%%% end with \\ to identify the column brake.
%
%%% ONE-COLUMN TABLE
%
%%t
%\begin{table}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table}
%
%%% TWO-COLUMN TABLE
%
%%t
%\begin{table*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{table*}
%
%%% LANDSCAPE TABLE
%
%%t
%\begin{sidewaystable*}[t]
%\caption{TEXT}
%\begin{tabular}{column = lcr}
%\tophline
%
%\middlehline
%
%\bottomhline
%\end{tabular}
%\belowtable{} % Table Footnotes
%\end{sidewaystable*}
%
%
%%% MATHEMATICAL EXPRESSIONS
%
%%% All papers typeset by Copernicus Publications follow the math typesetting regulations
%%% given by the IUPAC Green Book (IUPAC: Quantities, Units and Symbols in Physical Chemistry,
%%% 2nd Edn., Blackwell Science, available at: http://old.iupac.org/publications/books/gbook/green_book_2ed.pdf, 1993).
%%%
%%% Physical quantities/variables are typeset in italic font (t for time, T for Temperature)
%%% Indices which are not defined are typeset in italic font (x, y, z, a, b, c)
%%% Items/objects which are defined are typeset in roman font (Car A, Car B)
%%% Descriptions/specifications which are defined by itself are typeset in roman font (abs, rel, ref, tot, net, ice)
%%% Abbreviations from 2 letters are typeset in roman font (RH, LAI)
%%% Vectors are identified in bold italic font using \vec{x}
%%% Matrices are identified in bold roman font
%%% Multiplication signs are typeset using the LaTeX commands \times (for vector products, grids, and exponential notations) or \cdot
%%% The character * should not be applied as mutliplication sign
%
%
%%% EQUATIONS
%
%%% Single-row equation
%
%\begin{equation}
%
%\end{equation}
%
%%% Multiline equation
%
%\begin{align}
%& 3 + 5 = 8\\
%& 3 + 5 = 8\\
%& 3 + 5 = 8
%\end{align}
%
%
%%% MATRICES
%
%\begin{matrix}
%x & y & z\\
%x & y & z\\
%x & y & z\\
%\end{matrix}
%
%
%%% ALGORITHM
%
%\begin{algorithm}
%\caption{...}
%\label{a1}
%\begin{algorithmic}
%...
%\end{algorithmic}
%\end{algorithm}
%
%
%%% CHEMICAL FORMULAS AND REACTIONS
%
%%% For formulas embedded in the text, please use \chem{}
%
%%% The reaction environment creates labels including the letter R, i.e. (R1), (R2), etc.
%
%\begin{reaction}
%%% \rightarrow should be used for normal (one-way) chemical reactions
%%% \rightleftharpoons should be used for equilibria
%%% \leftrightarrow should be used for resonance structures
%\end{reaction}
%
%
%%% PHYSICAL UNITS
%%%
%%% Please use \unit{} and apply the exponential notation


\end{document}
